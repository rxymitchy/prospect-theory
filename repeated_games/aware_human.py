from .ProspectTheory import ProspectTheory
import numpy as np
import random
from scipy.special import softmax

class AwareHumanPTAgent:
    """
    Sophisticated Aware Human PT Agent
    Knows the game structure and uses PT to compute best responses

    Specifically, has access to opp reference point and uses that to calculate opp best reply to each of its actions
    Then computes the best reply to the opp's best replies. 

    Compared to learning human, no beliefs and no RL, just a best reply agent. 
    """

    def __init__(self, payoff_matrix, pt_params, action_size, state_size, agent_id=0, opp_params=None, ref_setting='Fixed', lambda_ref=0.95):
        self.payoff_matrix = payoff_matrix
        self.pt = ProspectTheory(**pt_params)
        print('AH PT PARAMS: ', pt_params)

        self.agent_id = agent_id  # 0 for row player, 1 for column player
        # Defaulted to 0.95, the number here is pretty arbitrary just the reference update parameter
        self.lam_r = lambda_ref
        self.ref_update_mode = ref_setting
        print('AH: ', self.ref_update_mode)

        self.ref_point = pt_params['r']
        self.tau = 0.1 # tie break threshold
        self.temperature = 1.3 # High value to encourage randomness

        # env parameters
        self.action_size = action_size
        self.opp_action_size = opp_params['opponent_action_size']
        self.state_size = state_size

        # Important to know whether to apply pt transformation
        self.opponent_type = opp_params['opponent_type']

        # Flag whether to apply pt transformation
        if self.opponent_type == "AI":
            self.opp_cpt = False

        else:
            self.opp_cpt = True
            # We are literally constructing the opponent's entire pt function, 
            # and replace it at every environment step
            self.opp_pt = ProspectTheory(**opp_params['opp_pt'])

        # track ties
        self.softmax_counter = 0 

    def get_opp_br(self, matrix):
        '''
        We start by tracking the opponent replies conditioned on each of our actions. 
        That is, if we play action 0, what will the opponent player, or if we play action 1, what will the opp play
        the return is a len(action_size) array of opp reply indices
        '''

        # Track the indices of the best reply to each of OUR actions
        opp_best_responses = np.zeros(self.action_size, dtype=int)

        # Iterate over our own actions
        for i in range(self.action_size):
            # Temp variable tracks the best response with our each set over OPP actions
            opp_best_value = float("-inf") # value for comparison
            opp_best_response = 0 # index to return
            # now we look at apponent replies to each of our actions
            for j in range(self.opp_action_size):
                opp_value = matrix[i, j, 1 - self.agent_id] # 1 - agent_id keeps this robust to both col/row

                # apply pt transformation if the opp is a pt player
                if self.opp_cpt:
                    opp_value = self.opp_pt.value_function(opp_value)

                # Maximizing action values (eps helps prevent near ties)
                if opp_value > opp_best_value + 1e-8:
                    opp_best_value = opp_value
                    opp_best_response = j

                # Tie Break logic (Maybe random is wrong here?)
                elif np.abs(opp_value - opp_best_value) <= 1e-8:
                    if random.random() < 0.5:
                        opp_best_value = opp_value
                        opp_best_response = j

            # Index in the best response we foound for action i
            opp_best_responses[i] = opp_best_response

        return opp_best_responses

    def get_best_response(self, matrix, opp_best_responses):
        '''
        Now we are looking through our responses and indexing into them with the opponent BRs. 
        The point is that we are assuming that the opp plays that best response, and then we select
        our best response based on the payoffs generated by the opp BR
        '''
        # One value for each of our actions
        best_vals = np.zeros(self.action_size)

        for i in range(self.action_size):
            # Use precalculated opp response
            opp_response = opp_best_responses[i]
            value = matrix[i, opp_response, self.agent_id] # agent id indexes row/col
            # Always PT transforming here â€” it is degenerate so no need for full lottery (please confirm this)
            # My thinking is that we are not randomizing over our actions ever, and now we have certainty
            # over the opp action, so probabilities are degenerate and value transform is all that matters
            value = self.pt.value_function(value)
            best_vals[i] = value

        # Get max value and second max val for tie breaks
        opt_a = np.argmax(best_vals) # best
        subopt_vals = best_vals.copy() # copy to prevent in place mutilation
        subopt_vals[opt_a] = float("-inf") # maksk best option
        subopt_a = np.argmax(subopt_vals) # get max of masked best option list
        # Tie breaks 
        gap = best_vals[opt_a] - best_vals[subopt_a] # find difference
        if gap < self.tau: # if difference is tiny, we call it a tie
            # Log tie break
            self.softmax_counter += 1

            # As defined in the paper's algorithm, normalized here to prevent logit explosions
            vals = best_vals - best_vals.max()
            probs = softmax(vals / self.temperature, axis = 0) # increase randomness
            best_response = np.random.choice(self.action_size, p=probs) # sample

        # if no tie, just return the best response
        else:
            best_response = opt_a

        return best_response

    def act(self, state=None):
        matrix = self.payoff_matrix
        # Make robust to col/row designation
        if self.agent_id == 1:
            matrix = matrix.transpose(1, 0, 2)


        # First we need to get the opp best responses to our actions 
        opp_best_responses = self.get_opp_br(matrix)

        # Now the decision matrix has gone from 2x2 -> 2x1. We plug in each opp response and 
        # argmax the best action we can take conditioned on how the opponent will reply
        player_best_response = self.get_best_response(matrix, opp_best_responses)

        return player_best_response

    def ref_update(self, payoff, state, opp_payoff):
        '''
        EMA handles gradual updates, Q says based on our knowledge whats the **best** that we can do,
        EMAOR just sets reference point conditioned on opp rewards (we had a conversation talking about the psychology
        of how good it *could* be)
        Fixed doesnt get updated, its fixed
        Still made sense to update the reference points for the AH, 
        ***technically this is learning though so maybe you want to handle it another way***
        '''
        # here it made sense to keep the reference point 
        if self.ref_update_mode == "EMA":
            self.ref_point = self.lam_r * self.ref_point + (1 - self.lam_r) * payoff

        # Here we dont need to maximize over Q values, we have the payoff matrix
        # We just select the agent id from the 3rd dimension of the payoff table
        # for reference the payoff tables are structured like this:
        #
        # np.array([
        #        [[-1, -1], [-3, 0]],   # C/C, C/D
        #        [[0, -3], [-2, -2]]    # D/C, D/D
        #    ])
        #
        # We take the pt transformation just like with LH
        elif self.ref_update_mode == 'Q':
            payoffs = self.payoff_matrix[:,:,self.agent_id].flatten()
            values = np.array([self.pt.value_function(po) for po in payoffs])
            self.ref_point = values.max()

        # same deal just over opp rewards
        elif self.ref_update_mode == 'EMAOR':
            self.ref_point = self.lam_r * self.ref_point + (1 - self.lam_r) * opp_payoff
        # Make sure to actually update the pt function
        self.pt.r = self.ref_point
